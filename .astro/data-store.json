[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.9","content-config-digest","bb0785e090f56c7b","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,33,34],"dealq-technical-breakdown",{"id":11,"data":13,"body":17,"filePath":18,"digest":19,"rendered":20,"legacyId":32},{"title":14,"description":15,"pubDate":16},"I shut down my start-up and open-sourced the codebase","a deep dive into my architecture choices",["Date","2025-10-14T00:00:00.000Z"],"# dealq technical breakdown\n\nContent coming soon...","src/content/blog/dealq-technical-breakdown.md","f3b30e8df0a532f6",{"html":21,"metadata":22},"\u003Ch1 id=\"dealq-technical-breakdown\">dealq technical breakdown\u003C/h1>\n\u003Cp>Content coming soon…\u003C/p>",{"headings":23,"localImagePaths":27,"remoteImagePaths":28,"frontmatter":29,"imagePaths":31},[24],{"depth":25,"slug":11,"text":26},1,"dealq technical breakdown",[],[],{"title":14,"description":15,"pubDate":30},["Date","2025-10-14T00:00:00.000Z"],[],"dealq-technical-breakdown.md","axiom",{"id":33,"data":35,"body":39,"filePath":40,"digest":41,"rendered":42,"legacyId":77},{"title":36,"description":37,"pubDate":38},"I wrote a neural network library in pure C (and you should too)","Pt. 1 of building undeniable technical ability",["Date","2026-02-03T00:00:00.000Z"],"Axiom is a neural network library I wrote in pure C that achieves **96.5% accuracy on MNIST**—with zero external dependencies. It's just ~1,000 lines of C and the standard library.\n\nI used an LLM to scaffold boilerplate (function definitions, etc, directed by me), but every line of logic is mine. The matrix multiplications, the backpropagation, the memory management are all handwritten.\n\nWhy? Because modern ML frameworks are black boxes, and it was time for me to go deeper.\n\nI began learning ML 100 days ago (I've been [documenting my progress daily on X](https://x.com/MehtaDontStop/status/2015997510330744843)). In the first ~92 days, I completed [Andrew Ng's DeepLearning specialization & three Kaggle comps](https://github.com/into-the-mehtaverse/machine-learning), built a [segmentation studio using the SAM model](https://github.com/into-the-mehtaverse/segmentation-studio), and wrote the [LSTM forward pass in python and pure C without imports](https://github.com/into-the-mehtaverse/lstm-no-imports). Over the last 8 days, I built Axiom to cement my knowledge and solidify my mastery over the fundamentals.\n\nI wasn't satisfied with using methods like Pytorch's \"loss.backward()\" and training with tensors without knowing what's behind the hood; abstraction removes boilerplate at the expense of learning. The only way to truly know is to build it yourself.\n\nQuick API usage example:\n\n### Build & Run\n\\`\\`\\`bash\nmake\n./build/main train --epochs 10 --lr 0.01\n\n```c\n// Define network: 784 → 128 (ReLU) → 10 (Softmax)\nAxiomNet* net = axiom_create();\naxiom_add(net, axiom_layer_dense(784, 128), LAYER_DENSE);\naxiom_add(net, axiom_activation_relu(), LAYER_ACTIVATION);\naxiom_add(net, axiom_layer_dense(128, 10), LAYER_DENSE);\naxiom_add(net, axiom_activation_softmax(), LAYER_ACTIVATION);\n\n// Train and save\naxiom_train(net, x_train, y_train, epochs, learning_rate, batch_size);\naxiom_save(net, \"mnist_model.bin\");\n```\nIf you want to run it with MNIST, add a data folder to the root, and within an MNIST subfolder, add the four MNIST files.\n\n---\n\n## Architecture\n\nAxiom is built in layers of abstraction, each depending only on what's below it:\n\n```\n┌─────────────────────────────────────┐\n│            AxiomNet API             │  ← Network orchestration, training loop\n├─────────────────────────────────────┤\n│   Dense │ Activations │ Optimizer   │  ← Layers with forward/backward passes\n├─────────────────────────────────────┤\n│         Loss Functions              │  ← Cross-entropy, MSE + gradients\n├─────────────────────────────────────┤\n│           Tensor Engine             │  ← Data, shapes, strides, matmul\n└─────────────────────────────────────┘\n```\n\nThe network itself is a linked list of layers. Each layer stores its own weights, gradients, and cached inputs (for backprop). Forward pass walks the list head-to-tail; backward pass reverses it.\n\nI purposely built this library modularly so that I can extend it and experiment with more layers / optimizers / methods. This library will serve as my learning space. Adding a new layer type means implementing `forward()` and `backward()`. The rest of the machinery stays untouched.\n\n---\n\n## Technical Decisions\n\n### Stride-Based Tensor Indexing\n\nI wanted to understand how a tensor actually works. In reality, it turned out to be simpler than I'd imagined: a tensor is just a blob of floats *plus metadata* that tells you how to interpret them. My `Tensor` struct stores:\n\n```c\ntypedef struct {\n    float* data;      // raw values\n    size_t* shape;    // dimensions [batch, features]\n    size_t* strides;  // elements to jump per dimension\n    size_t ndim;\n    size_t size;\n} Tensor;\n```\n\nStrides are the key insight. For a 2D tensor with shape `[3, 4]`, the strides are `[4, 1]`—meaning to move one row, you skip 4 elements; to move one column, you skip 1. Element `[i, j]` lives at `data[i * strides[0] + j * strides[1]]`.\n\nWhy does this matter? Strides let me change how data is *viewed* without copying it. A transpose is just swapping the strides and shape—the underlying data stays put, saving tremendous computational overhead. (My implementation does copy for simplicity, but the architecture supports the optimization.)\n\nEvery matrix operation—matmul, add, broadcast—uses stride-aware indexing. It's more verbose than flat indexing, but it's what makes the tensor engine general-purpose. When I'm working with large amounts of data with expensive operations, this matters immensely.\n\n### Cache-Optimal Matrix Multiplication\n\nMy first matmul implementation was the textbook triple loop:\n\n```c\n// Naive ijk ordering\nfor (int i = 0; i \u003C m; i++) {\n    for (int j = 0; j \u003C p; j++) {\n        for (int k = 0; k \u003C n; k++) {\n            C[i][j] += A[i][k] * B[k][j];\n        }\n    }\n}\n```\n\nI realized this was running way slower than what I was used to seeing in Pytorch. I asked my good friend Opus 4.5 for some hints as to why this was the case, and it told me to look into how caching works and how data is stored on the CPU.\n\nWhat I learned: the problem is memory access patterns. In row-major storage (how C lays out 2D arrays), values in the same row are stored next to each other sequentially. There's a limit to how much can be stored in layer of memory, and there's a difference in how fast each memory layer is. There are registers, which are the fastest / available for immediate access, then the L1 / L2 type caches, then the RAM, each of which is progressively slower to access. In fact, L1 cache access might be 2-3x CPU cycles (essentially how time is measured with CPU processes), whereas accessing RAM might be 200x CPU cycles. So, since `B[k][j]` with varying `k` in the inner loop jumps across rows—each access is a cache miss. Meaning you need to access RAM instead of L1 cache (for example) everytime you jump between rows, so it could be ~200x slower for these operations.\n\nThe fix is reordering the loops:\n\n```c\n// Cache-friendly ikj ordering\nfor (int i = 0; i \u003C m; i++) {\n    for (int k = 0; k \u003C n; k++) {\n        float a_ik = A[i * stride_a0 + k * stride_a1];\n        for (int j = 0; j \u003C p; j++) {\n            C[i * stride_c0 + j] += a_ik * B[k * stride_b0 + j];\n        }\n    }\n}\n```\n\nNow the inner loop walks `B` sequentially across columns and `C` sequentially across columns. Both are cache-friendly. While the math remains unchanged, the improvement is dramatic as we're now optimally utilizing the hardware.\n\n### Manual Memory Management & Backpropagation\n\nIn Python, you allocate objects and the garbage collector cleans up. In C, every `malloc` needs a corresponding `free`, and partial failures need careful unwinding:\n\n```c\nTensor* tensor_create(size_t* shape, size_t ndim) {\n    Tensor* t = malloc(sizeof(Tensor));\n    if (!t) return NULL;\n\n    t->data = malloc(total_size * sizeof(float));\n    if (!t->data) { free(t); return NULL; }\n\n    t->shape = malloc(ndim * sizeof(size_t));\n    if (!t->shape) { free(t->data); free(t); return NULL; }\n\n    t->strides = malloc(ndim * sizeof(size_t));\n    if (!t->strides) { free(t->shape); free(t->data); free(t); return NULL; }\n    // ...\n}\n```\n\nEvery allocation can fail. Every failure must clean up everything allocated before it. This cascading pattern repeats throughout the codebase.\n\nBackpropagation adds another dimension: intermediate values from the forward pass must be cached for the backward pass. My `DenseLayer` stores `input_cache`—the input tensor it saw during forward—because computing weight gradients requires it:\n\n```c\n// In backward pass:\n// grad_weights = input^T @ grad_output (chain rule)\nTensor* input_transposed = tensor_transpose(layer->input_cache);\nTensor* grad_weights = tensor_matmul(input_transposed, grad_output);\n```\n\nThe gradient with respect to weights is the outer product of the cached input and the incoming gradient. This is the chain rule made concrete. And because I'm managing memory manually, I have to remember to free `input_transposed` immediately after use, free old gradients before storing new ones, and free the cache when the layer is destroyed.\n\nI validated all of this with Leaks from macos. The final result is **zero memory leaks** across training and inference.\n\n---\n\n## What I'd Do Differently\n\n**GPU support.** The current CPU-only design limits me to toy datasets. Real neural network libraries use CUDA or Metal for parallelism. This would be a significant undertaking—essentially a rewrite of the tensor engine. Still, learning how the GPU code works is an extension of the base concepts (memory management, cache access, parallelism, etc), so I'm highly confident going into this with stronger intution.\n\n**More layers and optimizers.** Axiom only has dense layers, ReLU, and softmax. No convolutions, no dropout, no batch norm. The optimizer is vanilla SGD with a fixed learning rate—no momentum, no Adam. The architecture is modular enough that adding these is just a matter of implementing `forward()` and `backward()` for each new component.\n\n**Strided views for broadcasting.** My broadcast implementation copies data into a new tensor. A more efficient approach would use virtual strides—setting stride to 0 along broadcast dimensions so the same element is reused. I opted for the copy to keep moving as broadcasting rules are still new to me and I wanted to keep the project moving along.\n\n**Code cleanup.** There are hardcoded switch statements keyed on layer type scattered through the codebase. As layers multiply, this becomes a maintenance nightmare. The right fix is a vtable-style dispatch: each layer type provides function pointers for `forward`, `backward`, and `free`. The network code becomes layer-agnostic.\n\n---\n\n## Closing Thoughts\n\nBuilding Axiom has given me strong intuition on the fundamentals and further conviction in my technical ability.\n\nI now understand C not as syntax but as a way of thinking—where data lives, how it moves, what \"ownership\" means without a garbage collector. I understand backpropagation to the root level instead of API calling—caching the right values, applying the chain rule, flowing gradients backward through a graph.\n\nAnd I understand PyTorch. When I call `loss.backward()`, I know there's a graph of tensors, each caching its inputs, each computing gradients with respect to its parameters. The magic is just matrix multiplication and careful memory management, repeated a thousand times.\n\nThe black box is open.\n\n---\n\n*[View the repo on GitHub →](https://github.com/into-the-mehtaverse/axiom)*","src/content/blog/axiom.md","decc667973ff83b4",{"html":43,"metadata":44},"\u003Cp>Axiom is a neural network library I wrote in pure C that achieves \u003Cstrong>96.5% accuracy on MNIST\u003C/strong>—with zero external dependencies. It’s just ~1,000 lines of C and the standard library.\u003C/p>\n\u003Cp>I used an LLM to scaffold boilerplate (function definitions, etc, directed by me), but every line of logic is mine. The matrix multiplications, the backpropagation, the memory management are all handwritten.\u003C/p>\n\u003Cp>Why? Because modern ML frameworks are black boxes, and it was time for me to go deeper.\u003C/p>\n\u003Cp>I began learning ML 100 days ago (I’ve been \u003Ca href=\"https://x.com/MehtaDontStop/status/2015997510330744843\">documenting my progress daily on X\u003C/a>). In the first ~92 days, I completed \u003Ca href=\"https://github.com/into-the-mehtaverse/machine-learning\">Andrew Ng’s DeepLearning specialization &#x26; three Kaggle comps\u003C/a>, built a \u003Ca href=\"https://github.com/into-the-mehtaverse/segmentation-studio\">segmentation studio using the SAM model\u003C/a>, and wrote the \u003Ca href=\"https://github.com/into-the-mehtaverse/lstm-no-imports\">LSTM forward pass in python and pure C without imports\u003C/a>. Over the last 8 days, I built Axiom to cement my knowledge and solidify my mastery over the fundamentals.\u003C/p>\n\u003Cp>I wasn’t satisfied with using methods like Pytorch’s “loss.backward()” and training with tensors without knowing what’s behind the hood; abstraction removes boilerplate at the expense of learning. The only way to truly know is to build it yourself.\u003C/p>\n\u003Cp>Quick API usage example:\u003C/p>\n\u003Ch3 id=\"build--run\">Build &#x26; Run\u003C/h3>\n\u003Cp>```bash\nmake\n./build/main train —epochs 10 —lr 0.01\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Define network: 784 → 128 (ReLU) → 10 (Softmax)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">AxiomNet\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> net \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> axiom_create\u003C/span>\u003Cspan style=\"color:#E1E4E8\">();\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_add\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#B392F0\">axiom_layer_dense\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">784\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">), LAYER_DENSE);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_add\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#B392F0\">axiom_activation_relu\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(), LAYER_ACTIVATION);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_add\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#B392F0\">axiom_layer_dense\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">10\u003C/span>\u003Cspan style=\"color:#E1E4E8\">), LAYER_DENSE);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_add\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#B392F0\">axiom_activation_softmax\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(), LAYER_ACTIVATION);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Train and save\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_train\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, x_train, y_train, epochs, learning_rate, batch_size);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_save\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"mnist_model.bin\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>If you want to run it with MNIST, add a data folder to the root, and within an MNIST subfolder, add the four MNIST files.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"architecture\">Architecture\u003C/h2>\n\u003Cp>Axiom is built in layers of abstraction, each depending only on what’s below it:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>┌─────────────────────────────────────┐\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>│            AxiomNet API             │  ← Network orchestration, training loop\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├─────────────────────────────────────┤\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>│   Dense │ Activations │ Optimizer   │  ← Layers with forward/backward passes\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├─────────────────────────────────────┤\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>│         Loss Functions              │  ← Cross-entropy, MSE + gradients\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├─────────────────────────────────────┤\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>│           Tensor Engine             │  ← Data, shapes, strides, matmul\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>└─────────────────────────────────────┘\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The network itself is a linked list of layers. Each layer stores its own weights, gradients, and cached inputs (for backprop). Forward pass walks the list head-to-tail; backward pass reverses it.\u003C/p>\n\u003Cp>I purposely built this library modularly so that I can extend it and experiment with more layers / optimizers / methods. This library will serve as my learning space. Adding a new layer type means implementing \u003Ccode>forward()\u003C/code> and \u003Ccode>backward()\u003C/code>. The rest of the machinery stays untouched.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"technical-decisions\">Technical Decisions\u003C/h2>\n\u003Ch3 id=\"stride-based-tensor-indexing\">Stride-Based Tensor Indexing\u003C/h3>\n\u003Cp>I wanted to understand how a tensor actually works. In reality, it turned out to be simpler than I’d imagined: a tensor is just a blob of floats \u003Cem>plus metadata\u003C/em> that tells you how to interpret them. My \u003Ccode>Tensor\u003C/code> struct stores:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">typedef\u003C/span>\u003Cspan style=\"color:#F97583\"> struct\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    float*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> data;\u003C/span>\u003Cspan style=\"color:#6A737D\">      // raw values\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    size_t*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> shape;\u003C/span>\u003Cspan style=\"color:#6A737D\">    // dimensions [batch, features]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    size_t*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> strides;\u003C/span>\u003Cspan style=\"color:#6A737D\">  // elements to jump per dimension\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    size_t\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> ndim;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    size_t\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> size;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">} Tensor;\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Strides are the key insight. For a 2D tensor with shape \u003Ccode>[3, 4]\u003C/code>, the strides are \u003Ccode>[4, 1]\u003C/code>—meaning to move one row, you skip 4 elements; to move one column, you skip 1. Element \u003Ccode>[i, j]\u003C/code> lives at \u003Ccode>data[i * strides[0] + j * strides[1]]\u003C/code>.\u003C/p>\n\u003Cp>Why does this matter? Strides let me change how data is \u003Cem>viewed\u003C/em> without copying it. A transpose is just swapping the strides and shape—the underlying data stays put, saving tremendous computational overhead. (My implementation does copy for simplicity, but the architecture supports the optimization.)\u003C/p>\n\u003Cp>Every matrix operation—matmul, add, broadcast—uses stride-aware indexing. It’s more verbose than flat indexing, but it’s what makes the tensor engine general-purpose. When I’m working with large amounts of data with expensive operations, this matters immensely.\u003C/p>\n\u003Ch3 id=\"cache-optimal-matrix-multiplication\">Cache-Optimal Matrix Multiplication\u003C/h3>\n\u003Cp>My first matmul implementation was the textbook triple loop:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Naive ijk ordering\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; i \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> m; i\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> j \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; j \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> p; j\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> k \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; k \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> n; k\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">            C\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[i][j] \u003C/span>\u003Cspan style=\"color:#F97583\">+=\u003C/span>\u003Cspan style=\"color:#FFAB70\"> A\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[i][k] \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#FFAB70\"> B\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[k][j];\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>I realized this was running way slower than what I was used to seeing in Pytorch. I asked my good friend Opus 4.5 for some hints as to why this was the case, and it told me to look into how caching works and how data is stored on the CPU.\u003C/p>\n\u003Cp>What I learned: the problem is memory access patterns. In row-major storage (how C lays out 2D arrays), values in the same row are stored next to each other sequentially. There’s a limit to how much can be stored in layer of memory, and there’s a difference in how fast each memory layer is. There are registers, which are the fastest / available for immediate access, then the L1 / L2 type caches, then the RAM, each of which is progressively slower to access. In fact, L1 cache access might be 2-3x CPU cycles (essentially how time is measured with CPU processes), whereas accessing RAM might be 200x CPU cycles. So, since \u003Ccode>B[k][j]\u003C/code> with varying \u003Ccode>k\u003C/code> in the inner loop jumps across rows—each access is a cache miss. Meaning you need to access RAM instead of L1 cache (for example) everytime you jump between rows, so it could be ~200x slower for these operations.\u003C/p>\n\u003Cp>The fix is reordering the loops:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Cache-friendly ikj ordering\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; i \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> m; i\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> k \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; k \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> n; k\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        float\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> a_ik \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#FFAB70\"> A\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[i \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> stride_a0 \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> k \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> stride_a1];\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> j \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; j \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> p; j\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">            C\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[i \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> stride_c0 \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> j] \u003C/span>\u003Cspan style=\"color:#F97583\">+=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> a_ik \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#FFAB70\"> B\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[k \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> stride_b0 \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> j];\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Now the inner loop walks \u003Ccode>B\u003C/code> sequentially across columns and \u003Ccode>C\u003C/code> sequentially across columns. Both are cache-friendly. While the math remains unchanged, the improvement is dramatic as we’re now optimally utilizing the hardware.\u003C/p>\n\u003Ch3 id=\"manual-memory-management--backpropagation\">Manual Memory Management &#x26; Backpropagation\u003C/h3>\n\u003Cp>In Python, you allocate objects and the garbage collector cleans up. In C, every \u003Ccode>malloc\u003C/code> needs a corresponding \u003Ccode>free\u003C/code>, and partial failures need careful unwinding:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">Tensor\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#B392F0\"> tensor_create\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">size_t*\u003C/span>\u003Cspan style=\"color:#FFAB70\"> shape\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#F97583\">size_t\u003C/span>\u003Cspan style=\"color:#FFAB70\"> ndim\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    Tensor\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> t \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> malloc\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">sizeof\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(Tensor));\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">t) \u003C/span>\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> NULL\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    t->data \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> malloc\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(total_size \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#F97583\"> sizeof\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">float\u003C/span>\u003Cspan style=\"color:#E1E4E8\">));\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">t->data) { \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t); \u003C/span>\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> NULL\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    t->shape \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> malloc\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(ndim \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#F97583\"> sizeof\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">size_t\u003C/span>\u003Cspan style=\"color:#E1E4E8\">));\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">t->shape) { \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t->data); \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t); \u003C/span>\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> NULL\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    t->strides \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> malloc\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(ndim \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#F97583\"> sizeof\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">size_t\u003C/span>\u003Cspan style=\"color:#E1E4E8\">));\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">t->strides) { \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t->shape); \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t->data); \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t); \u003C/span>\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> NULL\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    // ...\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Every allocation can fail. Every failure must clean up everything allocated before it. This cascading pattern repeats throughout the codebase.\u003C/p>\n\u003Cp>Backpropagation adds another dimension: intermediate values from the forward pass must be cached for the backward pass. My \u003Ccode>DenseLayer\u003C/code> stores \u003Ccode>input_cache\u003C/code>—the input tensor it saw during forward—because computing weight gradients requires it:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// In backward pass:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// grad_weights = input^T @ grad_output (chain rule)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">Tensor\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> input_transposed \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> tensor_transpose\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(layer\u003C/span>\u003Cspan style=\"color:#F97583\">->\u003C/span>\u003Cspan style=\"color:#FFAB70\">input_cache\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">Tensor\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> grad_weights \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> tensor_matmul\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(input_transposed, grad_output);\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The gradient with respect to weights is the outer product of the cached input and the incoming gradient. This is the chain rule made concrete. And because I’m managing memory manually, I have to remember to free \u003Ccode>input_transposed\u003C/code> immediately after use, free old gradients before storing new ones, and free the cache when the layer is destroyed.\u003C/p>\n\u003Cp>I validated all of this with Leaks from macos. The final result is \u003Cstrong>zero memory leaks\u003C/strong> across training and inference.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"what-id-do-differently\">What I’d Do Differently\u003C/h2>\n\u003Cp>\u003Cstrong>GPU support.\u003C/strong> The current CPU-only design limits me to toy datasets. Real neural network libraries use CUDA or Metal for parallelism. This would be a significant undertaking—essentially a rewrite of the tensor engine. Still, learning how the GPU code works is an extension of the base concepts (memory management, cache access, parallelism, etc), so I’m highly confident going into this with stronger intution.\u003C/p>\n\u003Cp>\u003Cstrong>More layers and optimizers.\u003C/strong> Axiom only has dense layers, ReLU, and softmax. No convolutions, no dropout, no batch norm. The optimizer is vanilla SGD with a fixed learning rate—no momentum, no Adam. The architecture is modular enough that adding these is just a matter of implementing \u003Ccode>forward()\u003C/code> and \u003Ccode>backward()\u003C/code> for each new component.\u003C/p>\n\u003Cp>\u003Cstrong>Strided views for broadcasting.\u003C/strong> My broadcast implementation copies data into a new tensor. A more efficient approach would use virtual strides—setting stride to 0 along broadcast dimensions so the same element is reused. I opted for the copy to keep moving as broadcasting rules are still new to me and I wanted to keep the project moving along.\u003C/p>\n\u003Cp>\u003Cstrong>Code cleanup.\u003C/strong> There are hardcoded switch statements keyed on layer type scattered through the codebase. As layers multiply, this becomes a maintenance nightmare. The right fix is a vtable-style dispatch: each layer type provides function pointers for \u003Ccode>forward\u003C/code>, \u003Ccode>backward\u003C/code>, and \u003Ccode>free\u003C/code>. The network code becomes layer-agnostic.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"closing-thoughts\">Closing Thoughts\u003C/h2>\n\u003Cp>Building Axiom has given me strong intuition on the fundamentals and further conviction in my technical ability.\u003C/p>\n\u003Cp>I now understand C not as syntax but as a way of thinking—where data lives, how it moves, what “ownership” means without a garbage collector. I understand backpropagation to the root level instead of API calling—caching the right values, applying the chain rule, flowing gradients backward through a graph.\u003C/p>\n\u003Cp>And I understand PyTorch. When I call \u003Ccode>loss.backward()\u003C/code>, I know there’s a graph of tensors, each caching its inputs, each computing gradients with respect to its parameters. The magic is just matrix multiplication and careful memory management, repeated a thousand times.\u003C/p>\n\u003Cp>The black box is open.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cem>\u003Ca href=\"https://github.com/into-the-mehtaverse/axiom\">View the repo on GitHub →\u003C/a>\u003C/em>\u003C/p>",{"headings":45,"localImagePaths":72,"remoteImagePaths":73,"frontmatter":74,"imagePaths":76},[46,50,54,57,60,63,66,69],{"depth":47,"slug":48,"text":49},3,"build--run","Build & Run",{"depth":51,"slug":52,"text":53},2,"architecture","Architecture",{"depth":51,"slug":55,"text":56},"technical-decisions","Technical Decisions",{"depth":47,"slug":58,"text":59},"stride-based-tensor-indexing","Stride-Based Tensor Indexing",{"depth":47,"slug":61,"text":62},"cache-optimal-matrix-multiplication","Cache-Optimal Matrix Multiplication",{"depth":47,"slug":64,"text":65},"manual-memory-management--backpropagation","Manual Memory Management & Backpropagation",{"depth":51,"slug":67,"text":68},"what-id-do-differently","What I’d Do Differently",{"depth":51,"slug":70,"text":71},"closing-thoughts","Closing Thoughts",[],[],{"title":36,"description":37,"pubDate":75},["Date","2026-02-03T00:00:00.000Z"],[],"axiom.md"]