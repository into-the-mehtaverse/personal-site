[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.9","content-config-digest","bb0785e090f56c7b","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[null],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,56,57],"axiom",{"id":11,"data":13,"body":17,"filePath":18,"digest":19,"rendered":20,"legacyId":55},{"title":14,"description":15,"pubDate":16},"I wrote a neural network library in pure C (and you should too)","Pt. 2 of building undeniable technical ability",["Date","2026-02-03T00:00:00.000Z"],"Axiom is a neural network library I wrote in pure C that achieves **96.5% accuracy on MNIST**—with zero external dependencies. It's just ~1,000 lines of C and the standard library.\n\nI used an LLM to scaffold boilerplate (function definitions, etc, directed by me), but every line of logic is mine. The matrix multiplications, the backpropagation, the memory management are all handwritten.\n\nWhy? Because modern ML frameworks are black boxes, and it was time for me to go deeper.\n\nI began learning ML 100 days ago (I've been [documenting my progress daily on X](https://x.com/MehtaDontStop/status/2015997510330744843)). In the first ~92 days, I completed [Andrew Ng's DeepLearning specialization & three Kaggle comps](https://github.com/into-the-mehtaverse/machine-learning), built a [segmentation studio using the SAM model](https://github.com/into-the-mehtaverse/segmentation-studio), and wrote the [LSTM forward pass in python and pure C without imports](https://github.com/into-the-mehtaverse/lstm-no-imports). Over the last 8 days, I built Axiom to cement my knowledge and solidify my mastery over the fundamentals.\n\nI wasn't satisfied with using methods like Pytorch's \"loss.backward()\" and training with tensors without knowing what's behind the hood; abstraction removes boilerplate at the expense of deep learning (no pun intended). The only way to truly know is to build it yourself.\n\nQuick API usage example:\n\n### Build & Run\n```c\n// Define network: 784 → 128 (ReLU) → 10 (Softmax)\nAxiomNet* net = axiom_create();\naxiom_add(net, axiom_layer_dense(784, 128), LAYER_DENSE);\naxiom_add(net, axiom_activation_relu(), LAYER_ACTIVATION);\naxiom_add(net, axiom_layer_dense(128, 10), LAYER_DENSE);\naxiom_add(net, axiom_activation_softmax(), LAYER_ACTIVATION);\n\n// Train and save\naxiom_train(net, x_train, y_train, epochs, learning_rate, batch_size);\naxiom_save(net, \"mnist_model.bin\");\n```\n\n---\n\n## Architecture\n\nAxiom is built in layers of abstraction, each depending only on what's below it:\n\n```\n┌─────────────────────────────────────┐\n│            AxiomNet API             │  ← Network orchestration, training loop\n├─────────────────────────────────────┤\n│   Dense │ Activations │ Optimizer   │  ← Layers with forward/backward passes\n├─────────────────────────────────────┤\n│         Loss Functions              │  ← Cross-entropy, MSE + gradients\n├─────────────────────────────────────┤\n│           Tensor Engine             │  ← Data, shapes, strides, matmul\n└─────────────────────────────────────┘\n```\n\nThe network itself is a linked list of layers. Each layer stores its own weights, gradients, and cached inputs (for backprop). Forward pass walks the list head-to-tail; backward pass reverses it.\n\nI purposely built this library modularly so that I can extend it and experiment with more layers / optimizers / methods. This library will serve as my learning space. Adding a new layer type means implementing `forward()` and `backward()`. The rest of the machinery stays untouched.\n\n---\n\n## Technical Decisions\n\n### Stride-Based Tensor Indexing\n\nI wanted to understand how a tensor actually works. In reality, it turned out to be simpler than I'd imagined: a tensor is just a blob of floats *plus metadata* that tells you how to interpret them. My `Tensor` struct stores:\n\n```c\ntypedef struct {\n    float* data;      // raw values\n    size_t* shape;    // dimensions [batch, features]\n    size_t* strides;  // elements to jump per dimension\n    size_t ndim;\n    size_t size;\n} Tensor;\n```\n\nStrides are the key insight. For a 2D tensor with shape `[3, 4]`, the strides are `[4, 1]`—meaning to move one row, you skip 4 elements; to move one column, you skip 1. Element `[i, j]` lives at `data[i * strides[0] + j * strides[1]]`.\n\nWhy does this matter? Strides let me change how data is *viewed* without copying it. A transpose is just swapping the strides and shape—the underlying data stays put, saving tremendous computational overhead. (My implementation does copy for simplicity, but the architecture supports the optimization.)\n\nEvery matrix operation—matmul, add, broadcast—uses stride-aware indexing. It's more verbose than flat indexing, but it's what makes the tensor engine general-purpose. When I'm working with large amounts of data with expensive operations, this matters immensely.\n\n### Cache-Optimal Matrix Multiplication\n\nMy first matmul implementation was the textbook triple loop:\n\n```c\n// Naive ijk ordering\nfor (int i = 0; i \u003C m; i++) {\n    for (int j = 0; j \u003C p; j++) {\n        for (int k = 0; k \u003C n; k++) {\n            C[i][j] += A[i][k] * B[k][j];\n        }\n    }\n}\n```\n\nI realized this was running way slower than what I was used to seeing in Pytorch. I asked my good friend Opus 4.5 for some hints as to why this was the case, and it told me to look into how caching works and how data is stored on the CPU.\n\nWhat I learned: the problem is memory access patterns. In row-major storage (how C lays out 2D arrays), values in the same row are stored next to each other sequentially. There's a limit to how much can be stored in layer of memory, and there's a difference in how fast each memory layer is. There are registers, which are the fastest / available for immediate access, then the L1 / L2 type caches, then the RAM, each of which is progressively slower to access. In fact, L1 cache access might be 2-3x CPU cycles (essentially how time is measured with CPU processes), whereas accessing RAM might be 200x CPU cycles. So, since `B[k][j]` with varying `k` in the inner loop jumps across rows—each access is a cache miss. Meaning you need to access RAM instead of L1 cache (for example) everytime you jump between rows, so it could be ~200x slower for these operations.\n\nThe fix is reordering the loops:\n\n```c\n// Cache-friendly ikj ordering\nfor (int i = 0; i \u003C m; i++) {\n    for (int k = 0; k \u003C n; k++) {\n        float a_ik = A[i * stride_a0 + k * stride_a1];\n        for (int j = 0; j \u003C p; j++) {\n            C[i * stride_c0 + j] += a_ik * B[k * stride_b0 + j];\n        }\n    }\n}\n```\n\nNow the inner loop walks `B` sequentially across columns and `C` sequentially across columns. Both are cache-friendly. While the math remains unchanged, the improvement is dramatic as we're now optimally utilizing the hardware.\n\n### Manual Memory Management & Backpropagation\n\nIn Python, you allocate objects and the garbage collector cleans up. In C, every `malloc` needs a corresponding `free`, and partial failures need careful unwinding:\n\n```c\nTensor* tensor_create(size_t* shape, size_t ndim) {\n    Tensor* t = malloc(sizeof(Tensor));\n    if (!t) return NULL;\n\n    t->data = malloc(total_size * sizeof(float));\n    if (!t->data) { free(t); return NULL; }\n\n    t->shape = malloc(ndim * sizeof(size_t));\n    if (!t->shape) { free(t->data); free(t); return NULL; }\n\n    t->strides = malloc(ndim * sizeof(size_t));\n    if (!t->strides) { free(t->shape); free(t->data); free(t); return NULL; }\n    // ...\n}\n```\n\nEvery allocation can fail. Every failure must clean up everything allocated before it. This cascading pattern repeats throughout the codebase.\n\nBackpropagation adds another dimension: intermediate values from the forward pass must be cached for the backward pass. My `DenseLayer` stores `input_cache`—the input tensor it saw during forward—because computing weight gradients requires it:\n\n```c\n// In backward pass:\n// grad_weights = input^T @ grad_output (chain rule)\nTensor* input_transposed = tensor_transpose(layer->input_cache);\nTensor* grad_weights = tensor_matmul(input_transposed, grad_output);\n```\n\nThe gradient with respect to weights is the outer product of the cached input and the incoming gradient. This is the chain rule made concrete. And because I'm managing memory manually, I have to remember to free `input_transposed` immediately after use, free old gradients before storing new ones, and free the cache when the layer is destroyed.\n\nI validated all of this with Leaks from macos. The final result is **zero memory leaks** across training and inference.\n\n---\n\n## What I'd Do Differently\n\n**GPU support.** The current CPU-only design limits me to toy datasets. Real neural network libraries use CUDA or Metal for parallelism. This would be a significant undertaking—essentially a rewrite of the tensor engine. Still, learning how the GPU code works is an extension of the base concepts (memory management, cache access, parallelism, etc), so I'm highly confident going into this with stronger intution.\n\n**More layers and optimizers.** Axiom only has dense layers, ReLU, and softmax. No convolutions, no dropout, no batch norm. The optimizer is vanilla SGD with a fixed learning rate—no momentum, no Adam. The architecture is modular enough that adding these is just a matter of implementing `forward()` and `backward()` for each new component.\n\n**Strided views for broadcasting.** My broadcast implementation copies data into a new tensor. A more efficient approach would use virtual strides—setting stride to 0 along broadcast dimensions so the same element is reused. I opted for the copy to keep moving as broadcasting rules are still new to me and I wanted to keep the project moving along.\n\n**Code cleanup.** There are hardcoded switch statements keyed on layer type scattered through the codebase. As layers multiply, this becomes a maintenance nightmare. The right fix is a vtable-style dispatch: each layer type provides function pointers for `forward`, `backward`, and `free`. The network code becomes layer-agnostic.\n\n---\n\n## Closing Thoughts\n\nBuilding Axiom has given me strong intuition on the fundamentals and further conviction in my technical ability.\n\nI now understand C not as syntax but as a way of thinking—where data lives, how it moves, what \"ownership\" means without a garbage collector. I understand backpropagation to the root level instead of API calling—caching the right values, applying the chain rule, flowing gradients backward through a graph.\n\nAnd I understand PyTorch. When I call `loss.backward()`, I know there's a graph of tensors, each caching its inputs, each computing gradients with respect to its parameters. The magic is just matrix multiplication and careful memory management, repeated a thousand times.\n\nThe black box is open.\n\n---\n\n*[View the repo on GitHub →](https://github.com/into-the-mehtaverse/axiom)*","src/content/blog/axiom.md","430dfc1a10b4274e",{"html":21,"metadata":22},"\u003Cp>Axiom is a neural network library I wrote in pure C that achieves \u003Cstrong>96.5% accuracy on MNIST\u003C/strong>—with zero external dependencies. It’s just ~1,000 lines of C and the standard library.\u003C/p>\n\u003Cp>I used an LLM to scaffold boilerplate (function definitions, etc, directed by me), but every line of logic is mine. The matrix multiplications, the backpropagation, the memory management are all handwritten.\u003C/p>\n\u003Cp>Why? Because modern ML frameworks are black boxes, and it was time for me to go deeper.\u003C/p>\n\u003Cp>I began learning ML 100 days ago (I’ve been \u003Ca href=\"https://x.com/MehtaDontStop/status/2015997510330744843\">documenting my progress daily on X\u003C/a>). In the first ~92 days, I completed \u003Ca href=\"https://github.com/into-the-mehtaverse/machine-learning\">Andrew Ng’s DeepLearning specialization &#x26; three Kaggle comps\u003C/a>, built a \u003Ca href=\"https://github.com/into-the-mehtaverse/segmentation-studio\">segmentation studio using the SAM model\u003C/a>, and wrote the \u003Ca href=\"https://github.com/into-the-mehtaverse/lstm-no-imports\">LSTM forward pass in python and pure C without imports\u003C/a>. Over the last 8 days, I built Axiom to cement my knowledge and solidify my mastery over the fundamentals.\u003C/p>\n\u003Cp>I wasn’t satisfied with using methods like Pytorch’s “loss.backward()” and training with tensors without knowing what’s behind the hood; abstraction removes boilerplate at the expense of deep learning (no pun intended). The only way to truly know is to build it yourself.\u003C/p>\n\u003Cp>Quick API usage example:\u003C/p>\n\u003Ch3 id=\"build--run\">Build &#x26; Run\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Define network: 784 → 128 (ReLU) → 10 (Softmax)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">AxiomNet\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> net \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> axiom_create\u003C/span>\u003Cspan style=\"color:#E1E4E8\">();\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_add\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#B392F0\">axiom_layer_dense\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">784\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">), LAYER_DENSE);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_add\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#B392F0\">axiom_activation_relu\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(), LAYER_ACTIVATION);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_add\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#B392F0\">axiom_layer_dense\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">10\u003C/span>\u003Cspan style=\"color:#E1E4E8\">), LAYER_DENSE);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_add\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#B392F0\">axiom_activation_softmax\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(), LAYER_ACTIVATION);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Train and save\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_train\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, x_train, y_train, epochs, learning_rate, batch_size);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">axiom_save\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(net, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"mnist_model.bin\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Chr>\n\u003Ch2 id=\"architecture\">Architecture\u003C/h2>\n\u003Cp>Axiom is built in layers of abstraction, each depending only on what’s below it:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>┌─────────────────────────────────────┐\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>│            AxiomNet API             │  ← Network orchestration, training loop\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├─────────────────────────────────────┤\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>│   Dense │ Activations │ Optimizer   │  ← Layers with forward/backward passes\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├─────────────────────────────────────┤\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>│         Loss Functions              │  ← Cross-entropy, MSE + gradients\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├─────────────────────────────────────┤\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>│           Tensor Engine             │  ← Data, shapes, strides, matmul\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>└─────────────────────────────────────┘\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The network itself is a linked list of layers. Each layer stores its own weights, gradients, and cached inputs (for backprop). Forward pass walks the list head-to-tail; backward pass reverses it.\u003C/p>\n\u003Cp>I purposely built this library modularly so that I can extend it and experiment with more layers / optimizers / methods. This library will serve as my learning space. Adding a new layer type means implementing \u003Ccode>forward()\u003C/code> and \u003Ccode>backward()\u003C/code>. The rest of the machinery stays untouched.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"technical-decisions\">Technical Decisions\u003C/h2>\n\u003Ch3 id=\"stride-based-tensor-indexing\">Stride-Based Tensor Indexing\u003C/h3>\n\u003Cp>I wanted to understand how a tensor actually works. In reality, it turned out to be simpler than I’d imagined: a tensor is just a blob of floats \u003Cem>plus metadata\u003C/em> that tells you how to interpret them. My \u003Ccode>Tensor\u003C/code> struct stores:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">typedef\u003C/span>\u003Cspan style=\"color:#F97583\"> struct\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    float*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> data;\u003C/span>\u003Cspan style=\"color:#6A737D\">      // raw values\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    size_t*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> shape;\u003C/span>\u003Cspan style=\"color:#6A737D\">    // dimensions [batch, features]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    size_t*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> strides;\u003C/span>\u003Cspan style=\"color:#6A737D\">  // elements to jump per dimension\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    size_t\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> ndim;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    size_t\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> size;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">} Tensor;\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Strides are the key insight. For a 2D tensor with shape \u003Ccode>[3, 4]\u003C/code>, the strides are \u003Ccode>[4, 1]\u003C/code>—meaning to move one row, you skip 4 elements; to move one column, you skip 1. Element \u003Ccode>[i, j]\u003C/code> lives at \u003Ccode>data[i * strides[0] + j * strides[1]]\u003C/code>.\u003C/p>\n\u003Cp>Why does this matter? Strides let me change how data is \u003Cem>viewed\u003C/em> without copying it. A transpose is just swapping the strides and shape—the underlying data stays put, saving tremendous computational overhead. (My implementation does copy for simplicity, but the architecture supports the optimization.)\u003C/p>\n\u003Cp>Every matrix operation—matmul, add, broadcast—uses stride-aware indexing. It’s more verbose than flat indexing, but it’s what makes the tensor engine general-purpose. When I’m working with large amounts of data with expensive operations, this matters immensely.\u003C/p>\n\u003Ch3 id=\"cache-optimal-matrix-multiplication\">Cache-Optimal Matrix Multiplication\u003C/h3>\n\u003Cp>My first matmul implementation was the textbook triple loop:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Naive ijk ordering\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; i \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> m; i\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> j \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; j \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> p; j\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> k \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; k \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> n; k\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">            C\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[i][j] \u003C/span>\u003Cspan style=\"color:#F97583\">+=\u003C/span>\u003Cspan style=\"color:#FFAB70\"> A\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[i][k] \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#FFAB70\"> B\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[k][j];\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>I realized this was running way slower than what I was used to seeing in Pytorch. I asked my good friend Opus 4.5 for some hints as to why this was the case, and it told me to look into how caching works and how data is stored on the CPU.\u003C/p>\n\u003Cp>What I learned: the problem is memory access patterns. In row-major storage (how C lays out 2D arrays), values in the same row are stored next to each other sequentially. There’s a limit to how much can be stored in layer of memory, and there’s a difference in how fast each memory layer is. There are registers, which are the fastest / available for immediate access, then the L1 / L2 type caches, then the RAM, each of which is progressively slower to access. In fact, L1 cache access might be 2-3x CPU cycles (essentially how time is measured with CPU processes), whereas accessing RAM might be 200x CPU cycles. So, since \u003Ccode>B[k][j]\u003C/code> with varying \u003Ccode>k\u003C/code> in the inner loop jumps across rows—each access is a cache miss. Meaning you need to access RAM instead of L1 cache (for example) everytime you jump between rows, so it could be ~200x slower for these operations.\u003C/p>\n\u003Cp>The fix is reordering the loops:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Cache-friendly ikj ordering\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; i \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> m; i\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> k \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; k \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> n; k\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        float\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> a_ik \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#FFAB70\"> A\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[i \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> stride_a0 \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> k \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> stride_a1];\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> j \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; j \u003C/span>\u003Cspan style=\"color:#F97583\">&#x3C;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> p; j\u003C/span>\u003Cspan style=\"color:#F97583\">++\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">            C\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[i \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> stride_c0 \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> j] \u003C/span>\u003Cspan style=\"color:#F97583\">+=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> a_ik \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#FFAB70\"> B\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[k \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> stride_b0 \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> j];\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Now the inner loop walks \u003Ccode>B\u003C/code> sequentially across columns and \u003Ccode>C\u003C/code> sequentially across columns. Both are cache-friendly. While the math remains unchanged, the improvement is dramatic as we’re now optimally utilizing the hardware.\u003C/p>\n\u003Ch3 id=\"manual-memory-management--backpropagation\">Manual Memory Management &#x26; Backpropagation\u003C/h3>\n\u003Cp>In Python, you allocate objects and the garbage collector cleans up. In C, every \u003Ccode>malloc\u003C/code> needs a corresponding \u003Ccode>free\u003C/code>, and partial failures need careful unwinding:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">Tensor\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#B392F0\"> tensor_create\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">size_t*\u003C/span>\u003Cspan style=\"color:#FFAB70\"> shape\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#F97583\">size_t\u003C/span>\u003Cspan style=\"color:#FFAB70\"> ndim\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    Tensor\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> t \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> malloc\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">sizeof\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(Tensor));\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">t) \u003C/span>\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> NULL\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    t->data \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> malloc\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(total_size \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#F97583\"> sizeof\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">float\u003C/span>\u003Cspan style=\"color:#E1E4E8\">));\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">t->data) { \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t); \u003C/span>\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> NULL\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    t->shape \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> malloc\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(ndim \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#F97583\"> sizeof\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">size_t\u003C/span>\u003Cspan style=\"color:#E1E4E8\">));\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">t->shape) { \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t->data); \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t); \u003C/span>\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> NULL\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    t->strides \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> malloc\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(ndim \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#F97583\"> sizeof\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">size_t\u003C/span>\u003Cspan style=\"color:#E1E4E8\">));\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">t->strides) { \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t->shape); \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t->data); \u003C/span>\u003Cspan style=\"color:#B392F0\">free\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(t); \u003C/span>\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> NULL\u003C/span>\u003Cspan style=\"color:#E1E4E8\">; }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    // ...\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Every allocation can fail. Every failure must clean up everything allocated before it. This cascading pattern repeats throughout the codebase.\u003C/p>\n\u003Cp>Backpropagation adds another dimension: intermediate values from the forward pass must be cached for the backward pass. My \u003Ccode>DenseLayer\u003C/code> stores \u003Ccode>input_cache\u003C/code>—the input tensor it saw during forward—because computing weight gradients requires it:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// In backward pass:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// grad_weights = input^T @ grad_output (chain rule)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">Tensor\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> input_transposed \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> tensor_transpose\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(layer\u003C/span>\u003Cspan style=\"color:#F97583\">->\u003C/span>\u003Cspan style=\"color:#FFAB70\">input_cache\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">Tensor\u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> grad_weights \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> tensor_matmul\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(input_transposed, grad_output);\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The gradient with respect to weights is the outer product of the cached input and the incoming gradient. This is the chain rule made concrete. And because I’m managing memory manually, I have to remember to free \u003Ccode>input_transposed\u003C/code> immediately after use, free old gradients before storing new ones, and free the cache when the layer is destroyed.\u003C/p>\n\u003Cp>I validated all of this with Leaks from macos. The final result is \u003Cstrong>zero memory leaks\u003C/strong> across training and inference.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"what-id-do-differently\">What I’d Do Differently\u003C/h2>\n\u003Cp>\u003Cstrong>GPU support.\u003C/strong> The current CPU-only design limits me to toy datasets. Real neural network libraries use CUDA or Metal for parallelism. This would be a significant undertaking—essentially a rewrite of the tensor engine. Still, learning how the GPU code works is an extension of the base concepts (memory management, cache access, parallelism, etc), so I’m highly confident going into this with stronger intution.\u003C/p>\n\u003Cp>\u003Cstrong>More layers and optimizers.\u003C/strong> Axiom only has dense layers, ReLU, and softmax. No convolutions, no dropout, no batch norm. The optimizer is vanilla SGD with a fixed learning rate—no momentum, no Adam. The architecture is modular enough that adding these is just a matter of implementing \u003Ccode>forward()\u003C/code> and \u003Ccode>backward()\u003C/code> for each new component.\u003C/p>\n\u003Cp>\u003Cstrong>Strided views for broadcasting.\u003C/strong> My broadcast implementation copies data into a new tensor. A more efficient approach would use virtual strides—setting stride to 0 along broadcast dimensions so the same element is reused. I opted for the copy to keep moving as broadcasting rules are still new to me and I wanted to keep the project moving along.\u003C/p>\n\u003Cp>\u003Cstrong>Code cleanup.\u003C/strong> There are hardcoded switch statements keyed on layer type scattered through the codebase. As layers multiply, this becomes a maintenance nightmare. The right fix is a vtable-style dispatch: each layer type provides function pointers for \u003Ccode>forward\u003C/code>, \u003Ccode>backward\u003C/code>, and \u003Ccode>free\u003C/code>. The network code becomes layer-agnostic.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"closing-thoughts\">Closing Thoughts\u003C/h2>\n\u003Cp>Building Axiom has given me strong intuition on the fundamentals and further conviction in my technical ability.\u003C/p>\n\u003Cp>I now understand C not as syntax but as a way of thinking—where data lives, how it moves, what “ownership” means without a garbage collector. I understand backpropagation to the root level instead of API calling—caching the right values, applying the chain rule, flowing gradients backward through a graph.\u003C/p>\n\u003Cp>And I understand PyTorch. When I call \u003Ccode>loss.backward()\u003C/code>, I know there’s a graph of tensors, each caching its inputs, each computing gradients with respect to its parameters. The magic is just matrix multiplication and careful memory management, repeated a thousand times.\u003C/p>\n\u003Cp>The black box is open.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cem>\u003Ca href=\"https://github.com/into-the-mehtaverse/axiom\">View the repo on GitHub →\u003C/a>\u003C/em>\u003C/p>",{"headings":23,"localImagePaths":50,"remoteImagePaths":51,"frontmatter":52,"imagePaths":54},[24,28,32,35,38,41,44,47],{"depth":25,"slug":26,"text":27},3,"build--run","Build & Run",{"depth":29,"slug":30,"text":31},2,"architecture","Architecture",{"depth":29,"slug":33,"text":34},"technical-decisions","Technical Decisions",{"depth":25,"slug":36,"text":37},"stride-based-tensor-indexing","Stride-Based Tensor Indexing",{"depth":25,"slug":39,"text":40},"cache-optimal-matrix-multiplication","Cache-Optimal Matrix Multiplication",{"depth":25,"slug":42,"text":43},"manual-memory-management--backpropagation","Manual Memory Management & Backpropagation",{"depth":29,"slug":45,"text":46},"what-id-do-differently","What I’d Do Differently",{"depth":29,"slug":48,"text":49},"closing-thoughts","Closing Thoughts",[],[],{"title":14,"description":15,"pubDate":53},["Date","2026-02-03T00:00:00.000Z"],[],"axiom.md","dealq-technical-blog",{"id":56,"data":58,"body":62,"filePath":63,"digest":64,"rendered":65,"legacyId":123},{"title":59,"description":60,"pubDate":61},"I shut down my start-up and broke down how I built the codebase","Pt. 1 of building undeniable technical ability",["Date","2025-10-14T00:00:00.000Z"],"Let me introduce you to **DealQ**, the AI workflow platform I built for commercial real estate investors.\n\nI wrote this blog post to show the technical decision-making and problem-solving that went into building this codebase, what I've learned from the process, and how I'd approach things differently next time.\n\nAs the sole technical co-founder, I architected and coded the platform into production, from schema design to web app design to infra, and it was being used by multiple enterprise customers during our private beta. It was built with scale and future feature shipments in mind. The company has since wound down for reasons outside the scope of this article and I am no longer shipping feature updates.\n\nWith that being said, let's jump in.\n\nJump to:\n1. [What is DealQ](#what-is-dealq)\n2. [Technical Challenges / Accomplishments](#technical-challenges--accomplishments)\n3. [Tech Stack](#tech-stack)\n4. [System Architecture](#system-architecture)\n5. [Technical Accomplishment: Rent Roll Pipeline](#rent-roll-pipeline-design)\n6. [Areas For Improvement](#areas-for-improvement)\n\n## What is DealQ\n\nThe original vision for DealQ was a platform where CRE (commercial real estate) investors could access AI powered workflows to solve the most mundane and time-intensive due diligence tasks. Particularly, the one we chose to solve first is deal screening / underwriting.\n\n**What that means to you as someone who probably doesn't know anything about real estate**:\n\nA CRE investor typically receives dozens of deals per week. There are three primary documents per deal, all of which are highly variable and messy, usually PDF or Excel files, namely:\n\n1. The offering memorandum - a 20-40 page unstructured PDF which contains the investment narrative and high-level financial info.\n\n2. The rent roll - multi-page document that contains all the unit data and rents of the property\n\n3. The trailing twelve statement - essentially an income statement for the last twelve months at the property\n\nEvery real estate firm has highly paid analysts that extract and structure data from the above docs and insert them into the firm's proprietary Excel-based financial model. This process takes anywhere from 30 minutes to multiple hours per property. DealQ's underwriting workflow cut this process down to under 10 minutes.\n\n![demo-gif](/screenshots/demo-gif.gif)\n\n## Technical Challenges / Accomplishments\n\nI'd say there were two main buckets in terms of technical challenges / accomplishments for this codebase. Below, I'll write high-level what they were and the solutions I designed for them.\n\n### 1. **LLM-Powered Data Extraction Across Inconsistent Document Formats**\n- **The Challenge**: Customers need 100% accurate data in financial modelling. I needed to extract large chunks of messy numerical data from highly variable PDFs using LLMS while preventing hallucinations, missing information, managing context rot, and maintaining speed.\n- **Solution**: On the backend, I built a multi-stage pipeline utilizing a strategic mix of deterministic methods (regex patterns, structured parsing) and LLMs for intelligent data structuring, with concurrent processing for speed. On the frontend, I built a workflow that lets investors see the structured data output, compare accuracy against the source documents side-by-side and make changes if needed (our extraction accuracy during the beta was 99% across ~300 deals we tested).\n\n### 2. **Production-Ready Infrastructure**\n- **Challenge**: Architecting a scalable, maintainable, and fault-tolerant system ready for enterprise customers for the first time. Prior to DealQ, my engineering focus had been mobile consumer applications.\n- **Solution**: I deployed a containerized application with Docker, deployed on Digital Ocean with Caddy reverse proxy, CI/CD with GitHub Actions, optimistic upload patterns, background workers, RLS and rigorous authentication, type safety on frontend/backend, and async processing pipelines. I designed a modular architecture that could easily convert our human-in-the-loop workflows into agentic tool calls (more on this later).\n\n## Tech Stack\n\nNow, a quick overview of the tech stack.\n\n### Backend\nFastAPI (Python), LangChain for coordinating LLMs, PyMuPDF for reading docs, Redis for caching, OpenPyXl for working with excel files, Supabase for DB and storage, Stripe for billing\n\n### Frontend\nNext.js, Zustand, Tailwind, AG-grid for table views, Shad-cn / radix for primitives, Supabase auth\n\n### Infra\nDocker, Digital Ocean, Caddy, Github actions for ci/cd\n\n## System Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Frontend\"\n        A[Next.js App]\n    end\n\n    subgraph \"API Layer\"\n        B[FastAPI Routes]\n    end\n\n    subgraph \"Orchestration Layer\"\n        C[Upload Orchestrator]\n        D[Deal Orchestrator]\n        E[Billing Orchestrator]\n        F[Pipeline Orchestrator]\n    end\n\n    subgraph \"Services\"\n        G[Stripe, Document Processing, Caching, DB, Storage]\n    end\n\n    A --> B\n    B --> C\n    B --> D\n    B --> E\n    B --> F\n\n    C --> G\n    D --> G\n    E --> G\n    F --> G\n```\n\n## Architecture\n\n### 1. Backend Architecture\n\nThe backend follows a clean separation of concerns:\n- **Routes** → **Orchestrators** → **Services**\n\n**Routes** are thin and only handle HTTP concerns - parsing requests, validating input, and passing data to orchestrators.\n\n**Orchestrators** coordinate business logic by calling multiple services. For example, when processing a deal upload, the Upload Orchestrator coordinates the Document Processing Service (PDF/Excel extraction), AI Service (data structuring), and Storage Service (file management).\n\n**Services** are pure, stateless functions that handle specific domains. Each service can be independently tested and reused across different workflows. There's no overlap between services - each has a single responsibility.\n\nI decided on this modular architecture bc of two main benefits: (1) easy testing and maintenance since services are isolated, and (2) future flexibility to run AI workflows as individual tool calls for an agentic system. The orchestrator pattern provides a clean abstraction layer between HTTP endpoints and business logic.\n\n### 2. Frontend Architecture\n\nI chose **feature-based organization** on the frontend to keep related functionality together and improve maintainability as the team scales.\n\n**Directory Structure:**\n- **`/marketing`** - All pre-auth pages (landing, sign-in, pricing) separated from the main app\n- **`/features`** - Feature-specific components, stores, and logic (deals, verification, billing)\n- **`/components`** - Reusable UI primitives and app-wide components\n- **`/lib/api`** - Centralized API actions providing a clear interface to backend capabilities. All API actions are happening server-side with \"use server\".\n\n**Key Components:**\n- **Custom Document Viewers** - Built PDF and Excel viewers with gesture handling for desktop. Using custom-styled radix primitives for more complex components and ShadCN elsewhere.\n- **OM Viewer** (`@/features/deals/summary`) - Combines PDF viewer with AI-powered classification tooltips that link page numbers to extracted data points\n- **Feature Stores** - Zustand stores scoped to features with actions, selectors, and types for clean state management that can be maintained across workflows. (I love Zustand and use it for all my projects)\n\n## Rent Roll Pipeline Design\n\nHere's a deep dive on a specific challenge I encountered while building the extraction pipeline. I am including this section so you can get a sense of how I think about technical problems. I'll start by framing what we're trying to do, defining the criteria for success, and finally, you'll read a blurb on how I iterated to the final solution.\n\n**The Problem** Rent roll documents contain hundreds of units, duplicate information, inconsistent formatting between buildings and row-level inconsistencies as well. See the below screenshot as an example. The investor needs to boil this information down to just the occupying resident's information and current lease rent (no parking or discounts etc). We need to create a system that can handle all sorts of inconsistencies and get the accurate information out in a structured fashion with the same accuracy as a real estate analyst in a fraction of the time.\n\n![Rent Roll Screenshot](/screenshots/rent-roll-screenshot.png)\n\n### Criteria\n\nThe pipeline must be:\n1. **Accurate** - we need to make sure all the data that is pulled is correct\n2. **Complete** - the final data must include all the relevant units and exclude duplicates and totals\n3. **Consistent** - we should be getting the same result every time for the same document\n4. **Stable** - large properties should not time out the LLM calls\n5. **Fast** - we need to make sure this doesn't feel disruptive to the user's workflow by making them spend ten minutes on a loading screen\n\n### The Process\n\nI will walk you through my iterations building the rent roll pipeline. It always begins by extracting the raw text from the PDF using PyMuPDF with OCR as fallback or OpenPyXL if Excel file.\n\nFirst, I tried to one-shot the structuring by passing in the raw text with a strong prompt to see how far that would get. It appeared that up to ~5000 characters / 40 units it would succeed in returning all the units, but the rent numbers would be inconsistent (mixing up headers, etc). For any larger property or file, the LLM would return early and leave out units or it would time out and return an error.\n\nIn the second iteration, I tried to chunk the rent roll file and process concurrently in chunks of 5000 characters (respecting row / page boundaries). While this allowed handling of large properties / files, I ran into the issue of duplicates and totals being included since each chunk did not know what was included in the other chunks.\n\nThen, I tried sequential processing of chunks while updating the unit count based on what was listed in the offering memorandum and force return when the count was hit, but inconsistencies across data sources made this solution very brittle, and even in situations with perfect data, it was very slow.\n\nSo, in the final version of the pipeline (after a couple more iterations), I used a strategic mix of deterministic extraction and LLM calls. It goes as follows:\n\n1. Enumerate the raw text by pages / rows to give the LLM clear context.\n2. LLM Call: pass in enumerated raw text and ask to return a JSON which identifies start/end string of the relevant data, column headers and estimated unit count (or start/end row).\n3. LLM Call: As a JSON, Pass in the beginning of the file to X number of rows/characters after the start boundary, as well as X rows before the end boundary all the way to the end of the file. Have the LLM return a JSON which says if the first call was true or false, and if false, then provides the corrected boundary. This is for cases where long files return early or duplicates are being included, so that we are absolutely sure that the data boundaries are correct.\n4. Using regex, we extract only the relevant data in the backend based on the LLM-defined boundaries\n5. \"Smart chunking\" of the extracted relevant data that respects row and page boundaries and includes the column headers.\n6. Since we know all the raw text in the chunks is relevant, we process the chunks concurrently with LLMs\n7. Combine all the LLM outputs for the final result\n\nI tested this pipeline across hundreds of deals and refined the prompts until we achieved 99% accuracy across any type of deal or rent roll document. I also tested all the competing products which had AI rent roll extractors, and **DealQ was faster, more robust, and more accurate than anything else (as of October 2025)**\n\n### Additional win on time\n\nFor additional gain on processing time, instead of having the final LLM calls return an array of JSON objects like this:\n\n[ {\n    Unit: MLB-204,\n    Floor Plan: 1 Bed\n    Tenant Name: Bob Smith\n    Lease Start: 01/01/2025\n    Lease End: 12/31/2025\n    Rent Amount: 2000\n},\n{\n    Unit: MLB-206,\n    Floor Plan: 2 Bed\n    Tenant Name: Curly Jackson\n    Lease Start: 02/14/2024\n    Lease End: 02/13/2025\n    Rent Amount: 3500\n}\n]\n\nI asked them to return an array of arrays:\n\n[\n    [\n     MLB-204,\n     1 Bed\n     Bob Smith\n     01/01/2025\n     12/31/2025\n     2000\n    ],\n    [\n     MLB-206,\n     2 Bed\n     Curly Jackson\n     02/14/2024\n     02/13/2025\n     3500\n    ]\n]\n\n**This reduced token usage by 61% on average per chunk and drastically reduced the response time by more than 50%.**\n\n\n## Areas For Improvement\n\n1. Memory management\n\nAny application which processes large amounts of PDFs and excel files must have file streamining and memory limits. DealQ in its current form writes the file to a temp location while pulling the raw text. The Supabase Storage API's stremaing functionality was giving me trouble, so for the sake of shipping the private beta (very controlled), I put a file upload size limit on the front end, beefed up memory on the droplet, and pushed a ticket for fixing this before a full launch.\n\n2. Language choice & Type Safety\n\nFor DealQ, I built the backend with Python and the frontend with Typescript. I did this because I like coding in Python, but the multi-language codebase resulted in more complexity without any clear benefit. Looking back, I would have built fully Typescript across the codebase. Had any need arose for a library or service in another language, I'd have just deployed a microservice for the specific use case while keeping as much in TS as possible. Specifically, type safety and maintaing consistency in data models across the FE / BE became a nightmare. I used pydantic for backend typing, but defined type interfaces manually in component files / api action files on the frontend (an oversight, I should've used Zod but didn't know better when I started). Had I gone full Typescript, I could have built a shared types package in the root of the monorepo and had both apps require it as a dependency.\n\n3. Database management\n\nI did a terrible job at a managing DB migrations - I defined schemas, RLS policies, triggers, etc. in non-chronologically organized files with inconsistent naming and directly ran them in the SQL editor on Supabase while keeping records of what I ran in a /db folder in the backend. I get shudders when I think about this now. I should have built clean, reproducible, and reversible migrations with clear chronological naming and managed migrations effectively through scripts via the cli. I've changed the way I handle db management entirely after the mistakes on DealQ.\n\n---\n\n*[View the repo on GitHub →](https://github.com/into-the-mehtaverse/dealq-showcase)*","src/content/blog/dealq-technical-blog.md","b90a846638cd1e2e",{"html":66,"metadata":67},"\u003Cp>Let me introduce you to \u003Cstrong>DealQ\u003C/strong>, the AI workflow platform I built for commercial real estate investors.\u003C/p>\n\u003Cp>I wrote this blog post to show the technical decision-making and problem-solving that went into building this codebase, what I’ve learned from the process, and how I’d approach things differently next time.\u003C/p>\n\u003Cp>As the sole technical co-founder, I architected and coded the platform into production, from schema design to web app design to infra, and it was being used by multiple enterprise customers during our private beta. It was built with scale and future feature shipments in mind. The company has since wound down for reasons outside the scope of this article and I am no longer shipping feature updates.\u003C/p>\n\u003Cp>With that being said, let’s jump in.\u003C/p>\n\u003Cp>Jump to:\u003C/p>\n\u003Col>\n\u003Cli>\u003Ca href=\"#what-is-dealq\">What is DealQ\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"#technical-challenges--accomplishments\">Technical Challenges / Accomplishments\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"#tech-stack\">Tech Stack\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"#system-architecture\">System Architecture\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"#rent-roll-pipeline-design\">Technical Accomplishment: Rent Roll Pipeline\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"#areas-for-improvement\">Areas For Improvement\u003C/a>\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"what-is-dealq\">What is DealQ\u003C/h2>\n\u003Cp>The original vision for DealQ was a platform where CRE (commercial real estate) investors could access AI powered workflows to solve the most mundane and time-intensive due diligence tasks. Particularly, the one we chose to solve first is deal screening / underwriting.\u003C/p>\n\u003Cp>\u003Cstrong>What that means to you as someone who probably doesn’t know anything about real estate\u003C/strong>:\u003C/p>\n\u003Cp>A CRE investor typically receives dozens of deals per week. There are three primary documents per deal, all of which are highly variable and messy, usually PDF or Excel files, namely:\u003C/p>\n\u003Col>\n\u003Cli>\n\u003Cp>The offering memorandum - a 20-40 page unstructured PDF which contains the investment narrative and high-level financial info.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>The rent roll - multi-page document that contains all the unit data and rents of the property\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>The trailing twelve statement - essentially an income statement for the last twelve months at the property\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp>Every real estate firm has highly paid analysts that extract and structure data from the above docs and insert them into the firm’s proprietary Excel-based financial model. This process takes anywhere from 30 minutes to multiple hours per property. DealQ’s underwriting workflow cut this process down to under 10 minutes.\u003C/p>\n\u003Cp>\u003Cimg src=\"/screenshots/demo-gif.gif\" alt=\"demo-gif\">\u003C/p>\n\u003Ch2 id=\"technical-challenges--accomplishments\">Technical Challenges / Accomplishments\u003C/h2>\n\u003Cp>I’d say there were two main buckets in terms of technical challenges / accomplishments for this codebase. Below, I’ll write high-level what they were and the solutions I designed for them.\u003C/p>\n\u003Ch3 id=\"1-llm-powered-data-extraction-across-inconsistent-document-formats\">1. \u003Cstrong>LLM-Powered Data Extraction Across Inconsistent Document Formats\u003C/strong>\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>The Challenge\u003C/strong>: Customers need 100% accurate data in financial modelling. I needed to extract large chunks of messy numerical data from highly variable PDFs using LLMS while preventing hallucinations, missing information, managing context rot, and maintaining speed.\u003C/li>\n\u003Cli>\u003Cstrong>Solution\u003C/strong>: On the backend, I built a multi-stage pipeline utilizing a strategic mix of deterministic methods (regex patterns, structured parsing) and LLMs for intelligent data structuring, with concurrent processing for speed. On the frontend, I built a workflow that lets investors see the structured data output, compare accuracy against the source documents side-by-side and make changes if needed (our extraction accuracy during the beta was 99% across ~300 deals we tested).\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"2-production-ready-infrastructure\">2. \u003Cstrong>Production-Ready Infrastructure\u003C/strong>\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Challenge\u003C/strong>: Architecting a scalable, maintainable, and fault-tolerant system ready for enterprise customers for the first time. Prior to DealQ, my engineering focus had been mobile consumer applications.\u003C/li>\n\u003Cli>\u003Cstrong>Solution\u003C/strong>: I deployed a containerized application with Docker, deployed on Digital Ocean with Caddy reverse proxy, CI/CD with GitHub Actions, optimistic upload patterns, background workers, RLS and rigorous authentication, type safety on frontend/backend, and async processing pipelines. I designed a modular architecture that could easily convert our human-in-the-loop workflows into agentic tool calls (more on this later).\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"tech-stack\">Tech Stack\u003C/h2>\n\u003Cp>Now, a quick overview of the tech stack.\u003C/p>\n\u003Ch3 id=\"backend\">Backend\u003C/h3>\n\u003Cp>FastAPI (Python), LangChain for coordinating LLMs, PyMuPDF for reading docs, Redis for caching, OpenPyXl for working with excel files, Supabase for DB and storage, Stripe for billing\u003C/p>\n\u003Ch3 id=\"frontend\">Frontend\u003C/h3>\n\u003Cp>Next.js, Zustand, Tailwind, AG-grid for table views, Shad-cn / radix for primitives, Supabase auth\u003C/p>\n\u003Ch3 id=\"infra\">Infra\u003C/h3>\n\u003Cp>Docker, Digital Ocean, Caddy, Github actions for ci/cd\u003C/p>\n\u003Ch2 id=\"system-architecture\">System Architecture\u003C/h2>\n\u003Cdiv class=\"mermaid\">graph TB\n    subgraph \"Frontend\"\n        A[Next.js App]\n    end\n\n    subgraph \"API Layer\"\n        B[FastAPI Routes]\n    end\n\n    subgraph \"Orchestration Layer\"\n        C[Upload Orchestrator]\n        D[Deal Orchestrator]\n        E[Billing Orchestrator]\n        F[Pipeline Orchestrator]\n    end\n\n    subgraph \"Services\"\n        G[Stripe, Document Processing, Caching, DB, Storage]\n    end\n\n    A --> B\n    B --> C\n    B --> D\n    B --> E\n    B --> F\n\n    C --> G\n    D --> G\n    E --> G\n    F --> G\u003C/div>\n\u003Ch2 id=\"architecture\">Architecture\u003C/h2>\n\u003Ch3 id=\"1-backend-architecture\">1. Backend Architecture\u003C/h3>\n\u003Cp>The backend follows a clean separation of concerns:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Routes\u003C/strong> → \u003Cstrong>Orchestrators\u003C/strong> → \u003Cstrong>Services\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Routes\u003C/strong> are thin and only handle HTTP concerns - parsing requests, validating input, and passing data to orchestrators.\u003C/p>\n\u003Cp>\u003Cstrong>Orchestrators\u003C/strong> coordinate business logic by calling multiple services. For example, when processing a deal upload, the Upload Orchestrator coordinates the Document Processing Service (PDF/Excel extraction), AI Service (data structuring), and Storage Service (file management).\u003C/p>\n\u003Cp>\u003Cstrong>Services\u003C/strong> are pure, stateless functions that handle specific domains. Each service can be independently tested and reused across different workflows. There’s no overlap between services - each has a single responsibility.\u003C/p>\n\u003Cp>I decided on this modular architecture bc of two main benefits: (1) easy testing and maintenance since services are isolated, and (2) future flexibility to run AI workflows as individual tool calls for an agentic system. The orchestrator pattern provides a clean abstraction layer between HTTP endpoints and business logic.\u003C/p>\n\u003Ch3 id=\"2-frontend-architecture\">2. Frontend Architecture\u003C/h3>\n\u003Cp>I chose \u003Cstrong>feature-based organization\u003C/strong> on the frontend to keep related functionality together and improve maintainability as the team scales.\u003C/p>\n\u003Cp>\u003Cstrong>Directory Structure:\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>/marketing\u003C/code>\u003C/strong> - All pre-auth pages (landing, sign-in, pricing) separated from the main app\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>/features\u003C/code>\u003C/strong> - Feature-specific components, stores, and logic (deals, verification, billing)\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>/components\u003C/code>\u003C/strong> - Reusable UI primitives and app-wide components\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>/lib/api\u003C/code>\u003C/strong> - Centralized API actions providing a clear interface to backend capabilities. All API actions are happening server-side with “use server”.\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Key Components:\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Custom Document Viewers\u003C/strong> - Built PDF and Excel viewers with gesture handling for desktop. Using custom-styled radix primitives for more complex components and ShadCN elsewhere.\u003C/li>\n\u003Cli>\u003Cstrong>OM Viewer\u003C/strong> (\u003Ccode>@/features/deals/summary\u003C/code>) - Combines PDF viewer with AI-powered classification tooltips that link page numbers to extracted data points\u003C/li>\n\u003Cli>\u003Cstrong>Feature Stores\u003C/strong> - Zustand stores scoped to features with actions, selectors, and types for clean state management that can be maintained across workflows. (I love Zustand and use it for all my projects)\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"rent-roll-pipeline-design\">Rent Roll Pipeline Design\u003C/h2>\n\u003Cp>Here’s a deep dive on a specific challenge I encountered while building the extraction pipeline. I am including this section so you can get a sense of how I think about technical problems. I’ll start by framing what we’re trying to do, defining the criteria for success, and finally, you’ll read a blurb on how I iterated to the final solution.\u003C/p>\n\u003Cp>\u003Cstrong>The Problem\u003C/strong> Rent roll documents contain hundreds of units, duplicate information, inconsistent formatting between buildings and row-level inconsistencies as well. See the below screenshot as an example. The investor needs to boil this information down to just the occupying resident’s information and current lease rent (no parking or discounts etc). We need to create a system that can handle all sorts of inconsistencies and get the accurate information out in a structured fashion with the same accuracy as a real estate analyst in a fraction of the time.\u003C/p>\n\u003Cp>\u003Cimg src=\"/screenshots/rent-roll-screenshot.png\" alt=\"Rent Roll Screenshot\">\u003C/p>\n\u003Ch3 id=\"criteria\">Criteria\u003C/h3>\n\u003Cp>The pipeline must be:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>Accurate\u003C/strong> - we need to make sure all the data that is pulled is correct\u003C/li>\n\u003Cli>\u003Cstrong>Complete\u003C/strong> - the final data must include all the relevant units and exclude duplicates and totals\u003C/li>\n\u003Cli>\u003Cstrong>Consistent\u003C/strong> - we should be getting the same result every time for the same document\u003C/li>\n\u003Cli>\u003Cstrong>Stable\u003C/strong> - large properties should not time out the LLM calls\u003C/li>\n\u003Cli>\u003Cstrong>Fast\u003C/strong> - we need to make sure this doesn’t feel disruptive to the user’s workflow by making them spend ten minutes on a loading screen\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"the-process\">The Process\u003C/h3>\n\u003Cp>I will walk you through my iterations building the rent roll pipeline. It always begins by extracting the raw text from the PDF using PyMuPDF with OCR as fallback or OpenPyXL if Excel file.\u003C/p>\n\u003Cp>First, I tried to one-shot the structuring by passing in the raw text with a strong prompt to see how far that would get. It appeared that up to ~5000 characters / 40 units it would succeed in returning all the units, but the rent numbers would be inconsistent (mixing up headers, etc). For any larger property or file, the LLM would return early and leave out units or it would time out and return an error.\u003C/p>\n\u003Cp>In the second iteration, I tried to chunk the rent roll file and process concurrently in chunks of 5000 characters (respecting row / page boundaries). While this allowed handling of large properties / files, I ran into the issue of duplicates and totals being included since each chunk did not know what was included in the other chunks.\u003C/p>\n\u003Cp>Then, I tried sequential processing of chunks while updating the unit count based on what was listed in the offering memorandum and force return when the count was hit, but inconsistencies across data sources made this solution very brittle, and even in situations with perfect data, it was very slow.\u003C/p>\n\u003Cp>So, in the final version of the pipeline (after a couple more iterations), I used a strategic mix of deterministic extraction and LLM calls. It goes as follows:\u003C/p>\n\u003Col>\n\u003Cli>Enumerate the raw text by pages / rows to give the LLM clear context.\u003C/li>\n\u003Cli>LLM Call: pass in enumerated raw text and ask to return a JSON which identifies start/end string of the relevant data, column headers and estimated unit count (or start/end row).\u003C/li>\n\u003Cli>LLM Call: As a JSON, Pass in the beginning of the file to X number of rows/characters after the start boundary, as well as X rows before the end boundary all the way to the end of the file. Have the LLM return a JSON which says if the first call was true or false, and if false, then provides the corrected boundary. This is for cases where long files return early or duplicates are being included, so that we are absolutely sure that the data boundaries are correct.\u003C/li>\n\u003Cli>Using regex, we extract only the relevant data in the backend based on the LLM-defined boundaries\u003C/li>\n\u003Cli>“Smart chunking” of the extracted relevant data that respects row and page boundaries and includes the column headers.\u003C/li>\n\u003Cli>Since we know all the raw text in the chunks is relevant, we process the chunks concurrently with LLMs\u003C/li>\n\u003Cli>Combine all the LLM outputs for the final result\u003C/li>\n\u003C/ol>\n\u003Cp>I tested this pipeline across hundreds of deals and refined the prompts until we achieved 99% accuracy across any type of deal or rent roll document. I also tested all the competing products which had AI rent roll extractors, and \u003Cstrong>DealQ was faster, more robust, and more accurate than anything else (as of October 2025)\u003C/strong>\u003C/p>\n\u003Ch3 id=\"additional-win-on-time\">Additional win on time\u003C/h3>\n\u003Cp>For additional gain on processing time, instead of having the final LLM calls return an array of JSON objects like this:\u003C/p>\n\u003Cp>[ {\nUnit: MLB-204,\nFloor Plan: 1 Bed\nTenant Name: Bob Smith\nLease Start: 01/01/2025\nLease End: 12/31/2025\nRent Amount: 2000\n},\n{\nUnit: MLB-206,\nFloor Plan: 2 Bed\nTenant Name: Curly Jackson\nLease Start: 02/14/2024\nLease End: 02/13/2025\nRent Amount: 3500\n}\n]\u003C/p>\n\u003Cp>I asked them to return an array of arrays:\u003C/p>\n\u003Cp>[\n[\nMLB-204,\n1 Bed\nBob Smith\n01/01/2025\n12/31/2025\n2000\n],\n[\nMLB-206,\n2 Bed\nCurly Jackson\n02/14/2024\n02/13/2025\n3500\n]\n]\u003C/p>\n\u003Cp>\u003Cstrong>This reduced token usage by 61% on average per chunk and drastically reduced the response time by more than 50%.\u003C/strong>\u003C/p>\n\u003Ch2 id=\"areas-for-improvement\">Areas For Improvement\u003C/h2>\n\u003Col>\n\u003Cli>Memory management\u003C/li>\n\u003C/ol>\n\u003Cp>Any application which processes large amounts of PDFs and excel files must have file streamining and memory limits. DealQ in its current form writes the file to a temp location while pulling the raw text. The Supabase Storage API’s stremaing functionality was giving me trouble, so for the sake of shipping the private beta (very controlled), I put a file upload size limit on the front end, beefed up memory on the droplet, and pushed a ticket for fixing this before a full launch.\u003C/p>\n\u003Col start=\"2\">\n\u003Cli>Language choice &#x26; Type Safety\u003C/li>\n\u003C/ol>\n\u003Cp>For DealQ, I built the backend with Python and the frontend with Typescript. I did this because I like coding in Python, but the multi-language codebase resulted in more complexity without any clear benefit. Looking back, I would have built fully Typescript across the codebase. Had any need arose for a library or service in another language, I’d have just deployed a microservice for the specific use case while keeping as much in TS as possible. Specifically, type safety and maintaing consistency in data models across the FE / BE became a nightmare. I used pydantic for backend typing, but defined type interfaces manually in component files / api action files on the frontend (an oversight, I should’ve used Zod but didn’t know better when I started). Had I gone full Typescript, I could have built a shared types package in the root of the monorepo and had both apps require it as a dependency.\u003C/p>\n\u003Col start=\"3\">\n\u003Cli>Database management\u003C/li>\n\u003C/ol>\n\u003Cp>I did a terrible job at a managing DB migrations - I defined schemas, RLS policies, triggers, etc. in non-chronologically organized files with inconsistent naming and directly ran them in the SQL editor on Supabase while keeping records of what I ran in a /db folder in the backend. I get shudders when I think about this now. I should have built clean, reproducible, and reversible migrations with clear chronological naming and managed migrations effectively through scripts via the cli. I’ve changed the way I handle db management entirely after the mistakes on DealQ.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cem>\u003Ca href=\"https://github.com/into-the-mehtaverse/dealq-showcase\">View the repo on GitHub →\u003C/a>\u003C/em>\u003C/p>",{"headings":68,"localImagePaths":118,"remoteImagePaths":119,"frontmatter":120,"imagePaths":122},[69,72,75,78,81,84,87,90,93,96,97,100,103,106,109,112,115],{"depth":29,"slug":70,"text":71},"what-is-dealq","What is DealQ",{"depth":29,"slug":73,"text":74},"technical-challenges--accomplishments","Technical Challenges / Accomplishments",{"depth":25,"slug":76,"text":77},"1-llm-powered-data-extraction-across-inconsistent-document-formats","1. LLM-Powered Data Extraction Across Inconsistent Document Formats",{"depth":25,"slug":79,"text":80},"2-production-ready-infrastructure","2. Production-Ready Infrastructure",{"depth":29,"slug":82,"text":83},"tech-stack","Tech Stack",{"depth":25,"slug":85,"text":86},"backend","Backend",{"depth":25,"slug":88,"text":89},"frontend","Frontend",{"depth":25,"slug":91,"text":92},"infra","Infra",{"depth":29,"slug":94,"text":95},"system-architecture","System Architecture",{"depth":29,"slug":30,"text":31},{"depth":25,"slug":98,"text":99},"1-backend-architecture","1. Backend Architecture",{"depth":25,"slug":101,"text":102},"2-frontend-architecture","2. Frontend Architecture",{"depth":29,"slug":104,"text":105},"rent-roll-pipeline-design","Rent Roll Pipeline Design",{"depth":25,"slug":107,"text":108},"criteria","Criteria",{"depth":25,"slug":110,"text":111},"the-process","The Process",{"depth":25,"slug":113,"text":114},"additional-win-on-time","Additional win on time",{"depth":29,"slug":116,"text":117},"areas-for-improvement","Areas For Improvement",[],[],{"title":59,"description":60,"pubDate":121},["Date","2025-10-14T00:00:00.000Z"],[],"dealq-technical-blog.md"]